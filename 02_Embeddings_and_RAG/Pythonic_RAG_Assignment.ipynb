{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lElF3o5PR6ys"
      },
      "source": [
        "# Your First RAG Application\n",
        "\n",
        "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
        "\n",
        "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
        "\n",
        "> NOTE: This was done with Python 3.11.4.\n",
        "\n",
        "> NOTE: There might be [compatibility issues](https://github.com/wandb/wandb/issues/7683) if you're on NVIDIA driver >552.44 As an interim solution - you can rollback your drivers to the 552.44."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CtcL8P8R6yt"
      },
      "source": [
        "## Table of Contents:\n",
        "\n",
        "- Task 1: Imports and Utilities\n",
        "- Task 2: Documents\n",
        "- Task 3: Embeddings and Vectors\n",
        "- Task 4: Prompts\n",
        "- Task 5: Retrieval Augmented Generation\n",
        "  - 🚧 Activity #1: Augment RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dz6GYilR6yt"
      },
      "source": [
        "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vD8b016.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmC0KFtR6yt"
      },
      "source": [
        "## Task 1: Imports and Utility\n",
        "\n",
        "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Z1dyrG4hR6yt"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.text_utils import CSVFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "9OrFZRnER6yt"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jGnpQsR6yu"
      },
      "source": [
        "## Task 2: Documents\n",
        "\n",
        "We'll be concerning ourselves with this part of the flow in the following section:\n",
        "\n",
        "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFPWvRUR6yu"
      },
      "source": [
        "### Loading Source Documents\n",
        "\n",
        "So, first things first, we need some documents to work with.\n",
        "\n",
        "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
        "\n",
        "In this case, we're going to parse our text file into a single document in memory.\n",
        "\n",
        "Let's look at the relevant bits of the `TextFileLoader` class:\n",
        "\n",
        "```python\n",
        "def load_file(self):\n",
        "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
        "            self.documents.append(f.read())\n",
        "```\n",
        "\n",
        "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n",
        "\n",
        "> NOTE: We're using blogs from PMarca (Marc Andreessen) as our sample data. This data is largely irrelevant as we want to focus on the mechanisms of RAG, which includes out data's shape and quality - but not specifically what the contents of the data are. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2sUEuGR6yu",
        "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "474"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_loader = CSVFileLoader(\"data/at_map_points.csv\")\n",
        "documents = text_loader.load_documents()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV-tj5WFR6yu",
        "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'description': \"Night 1. First night in tent. No condensation, yay. Olivier's \"\n",
            "                \"sleeping pad didn't stay inflated, boo. Freezing temps all \"\n",
            "                'night, but got some sleep.',\n",
            " 'latitude': 34.6174878,\n",
            " 'longitude': -84.1988643,\n",
            " 'name': 'Black Gap Shelter',\n",
            " 'text': 'Location: Black Gap Shelter\\n'\n",
            "         'Description: Night 1. First night in tent. No condensation, yay. '\n",
            "         \"Olivier's sleeping pad didn't stay inflated, boo. Freezing temps all \"\n",
            "         'night, but got some sleep.',\n",
            " 'wkt': 'POINT (-84.1988643 34.6174878)'}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "pprint(documents[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlTvCzYR6yu"
      },
      "source": [
        "### Splitting Text Into Chunks\n",
        "\n",
        "As we can see, there is one massive document.\n",
        "\n",
        "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
        "\n",
        "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
        "\n",
        "For this toy example, we'll just split blindly on length.\n",
        "\n",
        ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
        ">\n",
        ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
        ">- \"document(s)\" : single (or more) text object(s)\n",
        ">- \"corpus\" : the combination of all of our documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6Voc0jR6yv"
      },
      "source": [
        "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMC4tsEmR6yv",
        "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
      },
      "outputs": [],
      "source": [
        "# 💡 For my updates, my personal data is already chunked,\n",
        "# so this code is no longer needed.\n",
        "# text_splitter = CharacterTextSplitter()\n",
        "# split_documents = text_splitter.split_texts(documents)\n",
        "# len(split_documents)\n",
        "\n",
        "# Maintain variable names to simplify following updates\n",
        "split_documents = documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2wKT0WLR6yv"
      },
      "source": [
        "Let's take a look at some of the documents we've managed to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcYMwWJoR6yv",
        "outputId": "20d69876-feca-4826-b4be-32915276987a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': 'Location: Max Epperson Shelter.\\nDescription: April 8 shelter. Night 0.',\n",
              "  'name': 'Max Epperson Shelter.',\n",
              "  'description': 'April 8 shelter. Night 0.',\n",
              "  'wkt': 'POINT (-84.2484845 34.5581409)',\n",
              "  'latitude': 34.5581409,\n",
              "  'longitude': -84.2484845},\n",
              " {'text': \"Location: Black Gap Shelter\\nDescription: Night 1. First night in tent. No condensation, yay. Olivier's sleeping pad didn't stay inflated, boo. Freezing temps all night, but got some sleep.\",\n",
              "  'name': 'Black Gap Shelter',\n",
              "  'description': \"Night 1. First night in tent. No condensation, yay. Olivier's sleeping pad didn't stay inflated, boo. Freezing temps all night, but got some sleep.\",\n",
              "  'wkt': 'POINT (-84.1988643 34.6174878)',\n",
              "  'latitude': 34.6174878,\n",
              "  'longitude': -84.1988643},\n",
              " {'text': 'Location: Lunch\\nDescription: Some small hail. Sub 40 degrees all day. 10/10 score from Olivier for the pulled pork packets.',\n",
              "  'name': 'Lunch',\n",
              "  'description': 'Some small hail. Sub 40 degrees all day. 10/10 score from Olivier for the pulled pork packets.',\n",
              "  'wkt': 'POINT (-84.23723 34.59144)',\n",
              "  'latitude': 34.59144,\n",
              "  'longitude': -84.23723}]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU-RFP_R6yv"
      },
      "source": [
        "## Task 3: Embeddings and Vectors\n",
        "\n",
        "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
        "\n",
        "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API Key\n",
        "\n",
        "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
        "\n",
        "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database\n",
        "\n",
        "Let's set up our vector database to hold all our documents and their embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQrfAR1R6yv"
      },
      "source": [
        "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
        "\n",
        "Let's look at our `VectorDatabase().__init__()`:\n",
        "\n",
        "```python\n",
        "def __init__(self, embedding_model: EmbeddingModel = None):\n",
        "        self.vectors = defaultdict(np.array)\n",
        "        self.embedding_model = embedding_model or EmbeddingModel()\n",
        "```\n",
        "\n",
        "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
        "\n",
        "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "> **Quick Info About `text-embedding-3-small`**:\n",
        "> - It has a context window of **8191** tokens\n",
        "> - It returns vectors with dimension **1536**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L273pRdeR6yv"
      },
      "source": [
        "#### ❓Question #1:\n",
        "\n",
        "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
        "\n",
        "1. Is there any way to modify this dimension?\n",
        "2. What technique does OpenAI use to achieve this?\n",
        "\n",
        "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #2!\n",
        "\n",
        "##### ✅ Answer:\n",
        "\n",
        "1. Yes, we can pass in the optional `dimensions` parameter to set a specific dimensionality.\n",
        "    \n",
        "    It was impossible to find information about if there's a limit on what dimensions you can pass, so I tried it out. Turns out you can't go larger than the default of 1536 (duh, I guess) and you do less than 1. (Also duh, but you gotta try it, right?) but what's interesting is that within that range you can choose any number for the dimensions, while asking an LLM tries to convince you that only specific numbers are allowed, likely because external libraries only support specific embedding sizes.\n",
        "\n",
        "2. Matryoshka Representation Learning (MRL) is the concept that allows us to essentially \"chop off\" the ends of embeddings and still maintain semantic meaning.\n",
        "\n",
        "    At a high level, this means that the further into our embedding we look, the more granular it will be in its representation. In reality, we have to do some normalization, we can't truly just slice off the end of an embedding, but this is a really cool feature considering that it used to be if you cut off part of an embedding, it lost its meaning entirely.\n",
        "\n",
        "##### ✅ End Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FZY7K3R6yv"
      },
      "source": [
        "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
        "\n",
        "```python\n",
        "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
        "        return await aget_embeddings(\n",
        "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSct6X0aR6yv"
      },
      "source": [
        "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
        "\n",
        "```python\n",
        "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
        "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
        "        for text, embedding in zip(list_of_text, embeddings):\n",
        "            self.insert(text, np.array(embedding))\n",
        "        return self\n",
        "```\n",
        "\n",
        "And that's all we need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "O4KoLbVDR6yv"
      },
      "outputs": [],
      "source": [
        "vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_documents(split_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZwaGvpR6yv"
      },
      "source": [
        "#### ❓Question #2:\n",
        "\n",
        "What are the benefits of using an `async` approach to collecting our embeddings?\n",
        "\n",
        "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!\n",
        "\n",
        "##### ✅ Answer:\n",
        "\n",
        "The benefit of using asynchronous code to create our embeddings is that we can make many concurrent calls to the embedding model at once, which explains why the above cell ran so quickly. Simply using `asyncio` isn't magic, though. We can follow the chain of method calls from `abuild_from_list` to `async_get_embeddings` where we finally see that we're calling our embedding model in batches, processing multiple batches at once using `asyncio.gather` (similar to `Promise.all()`). Neat!\n",
        "\n",
        "One might be tempted to think, why don't we run smaller batches concurrently to speed it up even more! This would be an interesting idea, but the batch processing done by OpenAI is more efficient than single requests or smaller batches. There might be some room for optimization, but generally speaking using OpenAI's native batch processing is a better call.\n",
        "\n",
        "##### ✅ End Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBdIt-xR6yw"
      },
      "source": [
        "So, to review what we've done so far in natural language:\n",
        "\n",
        "1. We load source documents\n",
        "2. We split those source documents into smaller chunks (documents)\n",
        "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
        "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vWANZyR6yw"
      },
      "source": [
        "### Semantic Similarity\n",
        "\n",
        "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
        "\n",
        "We're going to use the following process to achieve this in our toy example:\n",
        "\n",
        "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
        "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
        "3. We return a list of the top `k` closest vectors, with their text representations\n",
        "\n",
        "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
        "\n",
        "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
        "\n",
        "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d96uavR6yw",
        "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[({'text': 'Location: Bear Sighting\\nDescription: Our first bear sighting of the hike! It was a mama bear and two cubs, barely 0.1 or 0.2 miles away from our campsite for tonight, around 100 yards off the trail. -O',\n",
              "   'name': 'Bear Sighting',\n",
              "   'description': 'Our first bear sighting of the hike! It was a mama bear and two cubs, barely 0.1 or 0.2 miles away from our campsite for tonight, around 100 yards off the trail. -O',\n",
              "   'wkt': 'POINT (-82.79338 35.94822)',\n",
              "   'latitude': 35.94822,\n",
              "   'longitude': -82.79338},\n",
              "  np.float64(0.5973572421738066)),\n",
              " ({'text': 'Location: Bear\\nDescription: ',\n",
              "   'name': 'Bear',\n",
              "   'description': '',\n",
              "   'wkt': 'POINT (-80.01623 37.39295)',\n",
              "   'latitude': 37.39295,\n",
              "   'longitude': -80.01623},\n",
              "  np.float64(0.5604807365856969)),\n",
              " ({'text': \"Location: Whisky Hollow Shelter\\nDescription: June 30. Night 83. We had a long day today. There is some rain in the forecast for tomorrow so we wanted to bank a few extra miles to have a shorter day tomorrow. We also saw a bear. It looked like a adolescent. This is the 7th bear we've seen since we started. We also saw a big black snake that wouldn't get off the trail and let us pass. -O\",\n",
              "   'name': 'Whisky Hollow Shelter',\n",
              "   'description': \"June 30. Night 83. We had a long day today. There is some rain in the forecast for tomorrow so we wanted to bank a few extra miles to have a shorter day tomorrow. We also saw a bear. It looked like a adolescent. This is the 7th bear we've seen since we started. We also saw a big black snake that wouldn't get off the trail and let us pass. -O\",\n",
              "   'wkt': 'POINT (-77.9963 38.96946)',\n",
              "   'latitude': 38.96946,\n",
              "   'longitude': -77.9963},\n",
              "  np.float64(0.5155661523718009))]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.search_by_text(\"Did you encounter any bears?\", k=3, include_metadata=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehsfIiKR6yw"
      },
      "source": [
        "## Task 4: Prompts\n",
        "\n",
        "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
        "\n",
        "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
        "\n",
        "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpA0UveR6yw"
      },
      "source": [
        "### XYZRolePrompt\n",
        "\n",
        "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
        "\n",
        "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
        "\n",
        "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
        "\n",
        "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
        "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
        "\n",
        "The main idea is this:\n",
        "\n",
        "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
        "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
        "3. Then, you prompt the model with the true \"user\" message.\n",
        "\n",
        "In this example, we'll be forgoing the 2nd step for simplicities sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZ2KWKSR6yw"
      },
      "source": [
        "#### Utility Functions\n",
        "\n",
        "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbeJDDsR6yw"
      },
      "source": [
        "##### XYZRolePrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mojJSE3R6yw"
      },
      "source": [
        "Here we have our `system`, `user`, and `assistant` role prompts.\n",
        "\n",
        "Let's take a peek at what they look like:\n",
        "\n",
        "```python\n",
        "class BasePrompt:\n",
        "    def __init__(self, prompt):\n",
        "        \"\"\"\n",
        "        Initializes the BasePrompt object with a prompt template.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
        "\n",
        "    def format_prompt(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Formats the prompt string using the keyword arguments provided.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: The formatted prompt string\n",
        "        \"\"\"\n",
        "        matches = self._pattern.findall(self.prompt)\n",
        "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
        "\n",
        "    def get_input_variables(self):\n",
        "        \"\"\"\n",
        "        Gets the list of input variable names from the prompt string.\n",
        "\n",
        "        :return: List of input variable names\n",
        "        \"\"\"\n",
        "        return self._pattern.findall(self.prompt)\n",
        "```\n",
        "\n",
        "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
        "\n",
        "```python\n",
        "class RolePrompt(BasePrompt):\n",
        "    def __init__(self, prompt, role: str):\n",
        "        \"\"\"\n",
        "        Initializes the RolePrompt object with a prompt template and a role.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
        "        \"\"\"\n",
        "        super().__init__(prompt)\n",
        "        self.role = role\n",
        "\n",
        "    def create_message(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a message dictionary with a role and a formatted message.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: Dictionary containing the role and the formatted message\n",
        "        \"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
        "```\n",
        "\n",
        "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
        "\n",
        "```python\n",
        "class SystemRolePrompt(RolePrompt):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt, \"system\")\n",
        "```\n",
        "\n",
        "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D361R6sMR6yw"
      },
      "source": [
        "##### ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJVQ2Pm8R6yw"
      },
      "source": [
        "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
        "\n",
        "Let's take a peek at how that is constructed:\n",
        "\n",
        "```python\n",
        "class ChatOpenAI:\n",
        "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
        "        self.model_name = model_name\n",
        "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if self.openai_api_key is None:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
        "\n",
        "    def run(self, messages, text_only: bool = True):\n",
        "        if not isinstance(messages, list):\n",
        "            raise ValueError(\"messages must be a list\")\n",
        "\n",
        "        openai.api_key = self.openai_api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_name, messages=messages\n",
        "        )\n",
        "\n",
        "        if text_only:\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU7FfhIR6yw"
      },
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
        "\n",
        "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!\n",
        "\n",
        "##### ✅ Answer:\n",
        "\n",
        "Hands down, the best way to create more reproducible results is going to be through **prompt engineering**, and more specifically **few-shot prompting**.\n",
        "\n",
        "Providing specific instructions on how to behave, what tone to use, how specific to be, etc., is a good start to getting consistent results.\n",
        "\n",
        "Even with specific instructions, just like with humans, they are up to interpretation. The best way to get a human do to do something the way you want them to is to show them how to do it. It's the same with an LLM. If we give several consistent examples of what we want the LLM to do, it's going to be very likely to consistently follow those examples, given it's provided with a prompt that's appropriate to those examples.\n",
        "\n",
        "You could also consider things like top-p and temperature, but that's going to have unintended consequences unless you're goal is for your model to cut down on creativity and unique results. Either way, this should still be on top of few-shot prompting if the goal is true consistency.\n",
        "\n",
        "##### ✅ End Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wcjMLCR6yw"
      },
      "source": [
        "### Creating and Prompting OpenAI's `gpt-4o-mini`!\n",
        "\n",
        "Let's tie all these together and use it to prompt `gpt-4o-mini`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WIfpIot7R6yw"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "chat_openai = ChatOpenAI()\n",
        "user_prompt_template = \"{content}\"\n",
        "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
        "system_prompt_template = (\n",
        "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
        ")\n",
        "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
        "\n",
        "messages = [\n",
        "    system_role_prompt.create_message(expertise=\"Python\"),\n",
        "    user_role_prompt.create_message(\n",
        "        content=\"What is the best way to write a loop?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = chat_openai.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHo7lssNR6yw",
        "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best way to write a loop in Python often depends on the specific use case and the type of task you want to accomplish. However, I'll outline some best practices and common patterns for writing loops in Python:\n",
            "\n",
            "### For Loop\n",
            "\n",
            "A `for` loop is generally used when you want to iterate over a sequence (like a list, tuple, string, or range). Here’s an example:\n",
            "\n",
            "```python\n",
            "# Example: Print each item in a list\n",
            "my_list = [1, 2, 3, 4, 5]\n",
            "for item in my_list:\n",
            "    print(item)\n",
            "```\n",
            "\n",
            "### While Loop\n",
            "\n",
            "A `while` loop is useful when the number of iterations is not known beforehand and continues until a certain condition is met. Here’s an example:\n",
            "\n",
            "```python\n",
            "# Example: Countdown using while loop\n",
            "count = 5\n",
            "while count > 0:\n",
            "    print(count)\n",
            "    count -= 1\n",
            "```\n",
            "\n",
            "### Best Practices\n",
            "\n",
            "1. **Use Meaningful Variable Names**: Choose variable names that describe their role in the loop for better readability.\n",
            "   ```python\n",
            "   for number in range(5):\n",
            "       print(number)\n",
            "   ```\n",
            "\n",
            "2. **Keep Code Inside Loops Concise**: Aim to keep the code within your loop simple and avoid complicated logic unless necessary.\n",
            "\n",
            "3. **Avoid Hardcoding**: Whenever possible, use data structures or variables to make your loops adaptable.\n",
            "   ```python\n",
            "   fruits = ['apple', 'banana', 'cherry']\n",
            "   for fruit in fruits:\n",
            "       print(fruit)\n",
            "   ```\n",
            "\n",
            "4. **Use List Comprehensions**: For simple transformations, consider using list comprehensions instead of explicit loops to make your code more Pythonic and concise.\n",
            "   ```python\n",
            "   squares = [x**2 for x in range(10)]\n",
            "   ```\n",
            "\n",
            "5. **Break and Continue Statements**: Use `break` to exit the loop early when a condition is met, or `continue` to skip the current iteration.\n",
            "   ```python\n",
            "   for number in range(10):\n",
            "       if number == 5:\n",
            "           break  # Exit the loop when number is 5\n",
            "       print(number)\n",
            "   ```\n",
            "\n",
            "6. **Avoid Mutating a List Being Iterated**: If you are iterating over a list and need to remove items, consider creating a new list for the items you want to keep instead of modifying the list in place.\n",
            "\n",
            "By following these guidelines and choosing the appropriate type of loop according to your needs, you can write clear, efficient, and Pythonic code. If you have a specific scenario you’d like to discuss, feel free to share!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nxxhB2R6yy"
      },
      "source": [
        "## Task 5: Retrieval Augmented Generation\n",
        "\n",
        "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
        "\n",
        "There is much you could do here, many tweaks and improvements to be made!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "D1hamzGaR6yy"
      },
      "outputs": [],
      "source": [
        "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions about a 6-month thru-hike of the entire Appalachian Trail from Georgia to Maine, based on real trail journal entries.\n",
        "\n",
        "Instructions:\n",
        "- Only answer questions using information from the provided context (trail journal entries)\n",
        "- If the context doesn't contain relevant information, respond with \"I don't have information about that in the trail journal entries\"\n",
        "- Be accurate and cite specific locations, dates, or experiences from the context when possible\n",
        "- Keep responses {response_style} and {response_length}\n",
        "- Only use the provided context. Do not use external knowledge about the Appalachian Trail\n",
        "- Only provide answers when you are confident the context supports your response\n",
        "- When referencing locations, mention the specific place names and states when available\n",
        "- The trail journal includes GPS coordinates, so you can reference specific geographic locations\n",
        "- This is based on real experiences from a 2022 thru-hike, so focus on the personal experiences and observations recorded\"\"\"\n",
        "\n",
        "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
        "{context}\n",
        "\n",
        "Number of relevant sources found: {context_count}\n",
        "{similarity_scores}\n",
        "\n",
        "Question: {user_query}\n",
        "\n",
        "Please provide your answer based solely on the context above.\"\"\"\n",
        "\n",
        "rag_system_prompt = SystemRolePrompt(\n",
        "    RAG_SYSTEM_TEMPLATE,\n",
        "    strict=True,\n",
        "    defaults={\n",
        "        \"response_style\": \"concise\",\n",
        "        \"response_length\": \"brief\"\n",
        "    }\n",
        ")\n",
        "\n",
        "rag_user_prompt = UserRolePrompt(\n",
        "    RAG_USER_TEMPLATE,\n",
        "    strict=True,\n",
        "    defaults={\n",
        "        \"context_count\": \"\",\n",
        "        \"similarity_scores\": \"\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase, \n",
        "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "        self.response_style = response_style\n",
        "        self.include_scores = include_scores\n",
        "\n",
        "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
        "        # Retrieve relevant contexts\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k, include_metadata=True)\n",
        "        \n",
        "        context_prompt = \"\"\n",
        "        similarity_scores = []\n",
        "        \n",
        "        for i, (context, score) in enumerate(context_list, 1):\n",
        "            context_prompt += f\"[Source {i}]: {context}\\n\\n\"\n",
        "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
        "        \n",
        "        # Create system message with parameters\n",
        "        system_params = {\n",
        "            \"response_style\": self.response_style,\n",
        "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
        "        }\n",
        "        \n",
        "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
        "        \n",
        "        user_params = {\n",
        "            \"user_query\": user_query,\n",
        "            \"context\": context_prompt.strip(),\n",
        "            \"context_count\": len(context_list),\n",
        "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
        "        }\n",
        "        \n",
        "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
        "\n",
        "        return {\n",
        "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
        "            \"context\": context_list,\n",
        "            \"context_count\": len(context_list),\n",
        "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
        "            \"prompts_used\": {\n",
        "                \"system\": formatted_system_prompt,\n",
        "                \"user\": formatted_user_prompt\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Yes, there were several bear sightings during the hike. On one occasion, a mama bear and her two cubs were spotted barely 0.1 or 0.2 miles away from the campsite, approximately 100 yards off the trail. This was noted as the first bear sighting of the hike. Additionally, on July 7, at Birch Run Shelter, a huge bear was seen in the morning, and there was some humorous interaction as Olivier yelled at it, although it was not bothering them. The journal entry noted that the bear was simply passing by. \n",
            "\n",
            "These encounters highlight both sightings and reactions to bears along the trail.\n",
            "\n",
            "Context Count: 3\n",
            "Similarity Scores: ['Source 1: 0.572', 'Source 2: 0.532', 'Source 3: 0.515']\n"
          ]
        }
      ],
      "source": [
        "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai,\n",
        "    response_style=\"detailed\",\n",
        "    include_scores=True\n",
        ")\n",
        "\n",
        "# Possible questions:\n",
        "# What did you eat while you were on the AT?\n",
        "# Did you ever encounter any really crazy people?\n",
        "# Did you get any trail magic?\n",
        "\n",
        "result = rag_pipeline.run_pipeline(\n",
        "    \"Did you run into any bears?\",\n",
        "    k=3,\n",
        "    response_length=\"comprehensive\", \n",
        "    include_warnings=True,\n",
        "    confidence_required=True\n",
        ")\n",
        "\n",
        "print(f\"Response: {result['response']}\")\n",
        "print(f\"\\nContext Count: {result['context_count']}\")\n",
        "print(f\"Similarity Scores: {result['similarity_scores']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZIJI19uR6yz"
      },
      "source": [
        "#### ❓ Question #4:\n",
        "\n",
        "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
        "\n",
        "What is that strategy called?\n",
        "\n",
        "> NOTE: You can look through [\"Accessing GPT-3.5-turbo Like a Developer\"](https://colab.research.google.com/drive/1mOzbgf4a2SP5qQj33ZxTz2a01-5eXqk2?usp=sharing) for an answer to this question if you get stuck!\n",
        "\n",
        "##### ✅ Answer:\n",
        "\n",
        "The first thing I checked is the prompt we're already sending. We already have our `response_length` set to \"comprehensive\" and `response_style` set to \"detailed\". That means we have an instruction in our system message of:\n",
        "\n",
        "\"Keep responses detailed and comprehensive\"\n",
        "\n",
        "Given this, I would probably start with two approachs:\n",
        "\n",
        "1. Adding further instructions about what we mean by \"comprehensive.\" Is this multiple paragraphs? An overview followed by a breakdown of key points and a real-world example?\n",
        "\n",
        "2. Provide few-shot prompting. If we're looking to produce the same style of explanations over and over, we would benefit from showing several examples in addition to our more complete instructions.\n",
        "\n",
        "The strategies I've discussed broadly fall under \"prompt engineering,\" but more precisely I think \"few-shot prompting\" would have the greatest impact.\n",
        "\n",
        "##### ✅ End Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🏗️ Activity #1:\n",
        "\n",
        "Enhance your RAG application in some way! \n",
        "\n",
        "Suggestions are: \n",
        "\n",
        "- Allow it to work with PDF files\n",
        "- Implement a new distance metric\n",
        "- Add metadata support to the vector database\n",
        "\n",
        "While these are suggestions, you should feel free to make whatever augmentations you desire! \n",
        "\n",
        "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!\n",
        "\n",
        "> NOTE: If you're not sure where to start - ask Cursor (CMD/CTRL+L) to guide you through the changes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ✅ Start Activity #1\n",
        "\n",
        "##### Activity #1 Planning:\n",
        "\n",
        "- Allow it to work with PDF files\n",
        "    Maybe this means ingesting PDF files, maybe it means writing our output to a PDF file. Either way, file management doesn't sound like the most interesting use of time.\n",
        "\n",
        "- Implement a new distance metric\n",
        "    This could be interesting to see if we get any different results, but given the size of our dataset (rather small) I don't think it would make a big difference.\n",
        "\n",
        "- ⭐️ Add metadata support to the vector database\n",
        "    ~~This seems to be the most interesting suggestion, but it also comes with a challenge because the `PMarcaBlogs.txt` files is very unstructured. There's nothing cohesive enough here to create metadata from. It would be difficult to parse out titles or separators to determine what chunks belong to which blog.~~\n",
        "\n",
        "    ~~This would allow us to reference the blogs in the output and allow the user to dig deeper if they want to. Maybe we can even provide URLs, but we'll need to see what format we're getting the blogs in.~~\n",
        "\n",
        "    Given that the blogs aren't provided in a nice format and the online source seems to be only available in HTML, I'll update with my own structured data to allow for this implementation.\n",
        "\n",
        "    1. Add personal blog entries\n",
        "    2. Update document parsing to handle new format (CSV)\n",
        "    3. Update to store metadata (location, title of blog post)\n",
        "    4. Update prompts to be appropriate for new personal data\n",
        "\n",
        "Changes Made:\n",
        "- `data/`: `at_map_points.csv` added with personal blog entries\n",
        "- `text_utils.py`: Updated to parse a CSV file instead of a text file. Changed to `CSVFileLoader` class instead of `TextFileLoader`\n",
        "- `vectordatabase.py`: Updated to store metadata alongside embedding and blog text.\n",
        "\n",
        "This Notebook\n",
        "- Update to use new CSV Parser\n",
        "- Update to remove chunk splitting, since blog entries are already in suitable chunk sizes\n",
        "- Update to use new retrieval method that includes metadata\n",
        "- Update prompts to describe new task\n",
        "\n",
        "Running all cells top to bottom should work to demonstrate updates, but outputs have been left to show what's expected."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce393d9afcf427d9d352259c5d32678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6efd99f7d346e485b002fb0fa85cc7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dfb67c39958461da6071e4c19c3fa41",
            "value": 1
          }
        },
        "3a4ba348cb004f8ab7b2b1395539c81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ea5009dd16442cb5d8a0ac468e50a8",
            "placeholder": "​",
            "style": "IPY_MODEL_5f00135fe1044051a50ee5e841cbb8e3",
            "value": "0.018 MB of 0.018 MB uploaded\r"
          }
        },
        "3dfb67c39958461da6071e4c19c3fa41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e6efd99f7d346e485b002fb0fa85cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a8e24025594e5e9ff3b8581c344691": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f00135fe1044051a50ee5e841cbb8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb904e05ece143c79ecc4f20de482f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a4ba348cb004f8ab7b2b1395539c81b",
              "IPY_MODEL_1ce393d9afcf427d9d352259c5d32678"
            ],
            "layout": "IPY_MODEL_56a8e24025594e5e9ff3b8581c344691"
          }
        },
        "d2ea5009dd16442cb5d8a0ac468e50a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
